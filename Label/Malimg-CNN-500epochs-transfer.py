import os
import os.path
import glob
import time
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils, plot_model

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.cm as colormap

import numpy as np
import pandas as pd
np.random.seed(1)

from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from keras.applications.imagenet_utils import preprocess_input

from keras.callbacks import ModelCheckpoint,EarlyStopping
from keras.models import Sequential,Model
from keras.layers import Input,Flatten,Dense,Dropout,GlobalAveragePooling2D,Conv2D,MaxPooling2D

import keras
import sys
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

os.environ['CUDA_VISIBLE_DEVICES'] = '3'

# imagedir = "/home/limingzhao/new-14-famiy"
imagedir = sys.argv[1]
epoch = 500
# save_parameters = 'G:/学习资料/毕业设计/灰度图转换/image-classification-models/mine/Malimg-CNN-500epochs-transfer-0.5.h5'
#save_parameters = '/home/limingzhao/image-classification-models/mine/Malimg-CNN-500epochs-transfer-0.5.h5'
save_parameters =sys.argv[1]+"/Malimg-CNN-500epochs-transfer-0.5.h5"
X_train_drop = 0.2
# Experiment_results = "G:/学习资料/毕业设计/灰度图转换/image-classification-models/mine/"
#Experiment_results = "/home/limingzhao/image-classification-models/mine/"
os.mkdir(sys.argv[1]+"/png")
Experiment_results = sys.argv[1]+"/png/"
def preprocess_input(x):
    x /= 255.
    x -= 0.5
    x *= 2.
    return x

cur_dir = os.getcwd()
os.chdir(imagedir)  # the parent folder with sub-folders

# Get number of samples per family
list_fams = sorted(os.listdir(os.getcwd()), key=str.lower)  # vector of strings with family names
no_imgs = []  # No. of samples per family
for i in range(len(list_fams)):
    os.chdir(list_fams[i])
    len1 = len(glob.glob('*.png'))  # assuming the images are stored as 'png' 样本数向下取整
    no_imgs.append(len1)
    os.chdir('..')
num_samples = np.sum(no_imgs)  # total number of all samples

# Compute the labels
y = np.zeros(num_samples)
pos = 0
label = 0
for i in no_imgs:
    print ("Label:%2d\tFamily: %15s\tNumber of images: %d" % (label, list_fams[label], i))
    for j in range(i):
        y[pos] = label
        pos += 1
    label += 1
num_classes = label

# Compute the features
width, height,channels = (224,224,3)
X = np.zeros((num_samples, width, height, channels))
cnt = 0
list_paths = [] # List of image paths
print("Processing images ...")

#All_imgs = []

for i in range(len(list_fams)):
    pro = 1
    for img_file in glob.glob(list_fams[i]+'/*.png'):

#        All_imgs.append(img_file)  # 把所有图片名放入All_imgs数组中


        if pro > no_imgs[i]:
            break
        #print("[%d] Processing image: %s" % (cnt, img_file))
        list_paths.append(os.path.join(os.getcwd(),img_file))
        img = image.load_img(img_file, target_size=(224, 224))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis=0)
        x = preprocess_input(x)
        X[cnt] = x
        cnt += 1
        pro += 1
print("Images processed: %d" %(cnt))

os.chdir(cur_dir)

# Encoding classes (y) into integers (y_encoded) and then generating one-hot-encoding (Y)
encoder = LabelEncoder()
encoder.fit(y)
y_encoded = encoder.transform(y)
Y = np_utils.to_categorical(y_encoded)

# Creating base_model (VGG16 notop)
image_shape = (224, 224, 3)
base_model = VGG16(weights='imagenet', input_shape=image_shape, include_top=False)
#base_model.summary()

#filename = '/home/mayixuan/mal-visual/extracted-features/test.npy'
#if os.path.exists(filename):
#    print("Loading VGG16 extracted features from %s ..." %(filename))
#    vgg16features = np.load(filename)
#else:
#    print("Extracting features from VGG16 layers ...")
#    vgg16features = base_model.predict(X)
#    print("Saving VGG16 extracted features into %s ..." %(filename))
#    np.save(filename, vgg16features)

# Create stratified k-fold subsets
kfold = 2  # no. of folds
skf = StratifiedKFold(kfold, shuffle=True, random_state=1)
skfind = [None] * kfold  # skfind[i][0] -> train indices, skfind[i][1] -> test indices
cnt = 0
for index in skf.split(X, y):
    skfind[cnt] = index
    cnt += 1

# Training top_model and saving min training loss weights
num_epochs = epoch
history = []
conf_mat = np.zeros((len(list_fams), len(list_fams)))  # Initializing the Confusion Matrix
checkpointer = ModelCheckpoint(
    filepath=save_parameters,
    monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')
for i in range(kfold):
    train_indices = skfind[i][0]
    test_indices = skfind[i][1]
#   X_train = vgg16features[train_indices]
    X_train = X[train_indices]
    Y_train = Y[train_indices]
#   X_test = vgg16features[test_indices]
    X_test = X[test_indices]
    Y_test = Y[test_indices]
    y_test = y[test_indices]

#    All_imgs_fold = []
#    for j in range(len(X_test)):
#        for k in range(len(X)):
#            if (X_test[j]-X[k]).any() == False:
#                All_imgs_fold.append(All_imgs[k])
#                k = 0
#                break



    X_train, X_drop, Y_train, Y_drop = train_test_split(X_train, Y_train, test_size=X_train_drop, random_state=0)
    # 仅取训练集的10%用于训练，测试集统一

    x = base_model.output
    x = Flatten(name='flatten')(x)  # input_shape=(7,7,512)
    x = Dense(units=2048, activation='relu', name='FC-1')(x)
    x = Dense(units=2048, activation='relu', name='FC-2')(x)
    x = Dropout(0.4)(x)
    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)

    vgg_model = Model(inputs=base_model.input, outputs=predictions)  # 网络链接要注意
#    print(vgg_model)
#    vgg_model.summary()

#    print(len(vgg_model.layers))  # 24层
#    dict_layers = dict([(layer.name, layer) for layer in vgg_model.layers])
#    print(len(dict_layers))  # 24
#   plot_model(vgg_model, to_file='vgg16_fc.png', show_shapes=True)

    # 冻结conv block5之前的所有卷积层权重，不包括输入层
    for layer in vgg_model.layers[1:15]:
#       print(layer.name, layer)
        layer.trainable = False
    vgg_model.compile(optimizer=keras.optimizers.SGD(lr=5e-6, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])

    start = time.time()
    h = vgg_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=num_epochs,
                      batch_size=32, verbose=1, callbacks=[checkpointer])
    end = time.time()
    history.append(h)

    start_2 = time.time()
    y_prob = vgg_model.predict(X_test, verbose=0)  # Testing
    y_pred = np.argmax(y_prob, axis=1)
    end_2 = time.time()
    print("[%d] Test acurracy: %.4f (%.4f s)" % (i, accuracy_score(y_test, y_pred), end - start))
    print("Prediction %d samples use time : (%.4f s)" % (len(X_test), end_2 - start_2))
    cm = confusion_matrix(y_test, y_pred)  # Compute confusion matrix for this fold
    conf_mat = conf_mat + cm  # Compute global confusion matrix




    # 将图片名、ground truth label、predict label放入excel表格中（分别为1/2/3列）
#    data = pd.DataFrame({'name': All_imgs_fold,
#                         'label': y_test,
#                         'predict label': y_pred})

#    writer = pd.ExcelWriter('predict-process-test-100epoch-result-%d.xlsx' % (i+1))
#    data.to_excel(writer, 'page_1')
#    writer.save()

#    writer.close()
#    print('DataFrame is written successfully to the Excel File.')





# Computing the average accuracy
avg_acc = np.trace(conf_mat)/sum(no_imgs)
print("Average acurracy: %.4f" %(avg_acc))


# plot the graph
save_path = Experiment_results



print("Plotting the accuracy")
def plot_acc(history):
    figure = plt.gcf()
    figure.set_size_inches(14, 6)
    ax = plt.subplot()
    #plt.title('Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))
    for i in range(len(history)):
        color=next(colors)
        plt.plot(history[i].history['accuracy'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)
        plt.plot(history[i].history['val_accuracy'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)
    x1, x2, y1, y2 = plt.axis()
    plt.axis((x1, x2, 0.0, 1.0))
    plt.legend()
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    plt.grid(True)
    plt.savefig(save_path + "accuracy.png",dpi=300)
    plt.show()
plot_acc(history)

plt.clf()

print("Plotting the loss")
def plot_loss(history):
    figure = plt.gcf()
    figure.set_size_inches(14, 6)
    ax = plt.subplot()
    #plt.title('Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))
    for i in range(len(history)):
        color=next(colors)
        plt.plot(history[i].history['loss'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)
        # 处理过大的验证损失点
        for j in range(len(history[i].history['val_loss'])):
            if (history[i].history['val_loss'])[j] > 5:
                (history[i].history['val_loss'])[j] = 5
        plt.plot(history[i].history['val_loss'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)
    plt.legend()
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    plt.grid(True)
    plt.savefig(save_path + "loss.png", dpi=300)
    plt.show()
plot_loss(history)

plt.clf()


print("Plotting the confusion matrix")
figure = plt.gcf()
figure.set_size_inches(18, 12)
plt.imshow(conf_mat,interpolation='nearest',cmap='coolwarm')
for row in range(len(list_fams)):
    for col in range(len(list_fams)):
        plt.annotate(str(int(conf_mat[row][col])),xy=(col,row),ha='center',va='center')
plt.xticks(range(len(list_fams)),list_fams,rotation=90,fontsize=10)
plt.yticks(range(len(list_fams)),list_fams,fontsize=10)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.savefig(save_path + "confusion_matrix.png", dpi=300)
plt.show()

plt.clf()


print("Plotting the confusion matrix normalized")
conf_mat_norm = conf_mat/no_imgs  # Normalizing the confusion matrix
conf_mat_norm = np.around(conf_mat_norm,decimals=2)  # rounding to display in figure
figure = plt.gcf()
figure.set_size_inches(20, 12)
plt.imshow(conf_mat_norm,interpolation='nearest',cmap='seismic')
for row in range(len(list_fams)):
    for col in range(len(list_fams)):
        plt.annotate(str(conf_mat_norm[row][col]),xy=(col,row),ha='center',va='center')
plt.xticks(range(len(list_fams)),list_fams,rotation=90,fontsize=10)
plt.yticks(range(len(list_fams)),list_fams,fontsize=10)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.savefig(save_path + "confusion_matrix_normalized.png", dpi=300)
plt.show()

plt.clf()

import seaborn as sns

print("Plotting the gray confusion matrix normalized")
conf_mat_norm = conf_mat/np.sum(conf_mat,axis=1)  # Normalizing the confusion matrix
conf_mat_norm = np.around(conf_mat_norm,decimals=2)  # rounding to display in figure

figure = plt.gcf()
figure.set_size_inches(20, 12)
sns.set(font_scale=1.25)
hm = sns.heatmap(conf_mat_norm, cbar=False, annot=True, square=True,
                 fmt='.2f', annot_kws={'size': 9}, linewidth = 0.1, cmap = 'binary',
                 yticklabels=list_fams, xticklabels=list_fams)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.savefig(save_path + "gray_confusion_matrix_normalized.png", dpi=300)
plt.show()

plt.clf()

import matplotlib.ticker as ticker
plt.style.use('seaborn-whitegrid')

print("Plotting the mean accuracy")
def plot_mean_acc(history):
    train_scores = np.zeros((len(history), len(history[0].history['accuracy'])))
    for fold in range(len(history)):
        train_scores[fold] = history[fold].history['accuracy']
    test_scores = np.zeros((len(history), len(history[0].history['val_accuracy'])))
    for fold in range(len(history)):
        test_scores[fold] = history[fold].history['val_accuracy']
    epochs = np.linspace(0, len(history[0].history['accuracy']), len(history[0].history['accuracy']))
    train_scores_mean = np.mean(train_scores, axis=0)
    train_scores_std = np.std(train_scores, axis=0)
    test_scores_mean = np.mean(test_scores, axis=0)
    test_scores_std = np.std(test_scores, axis=0)

    figsize = (14, 6)
    text_fontsize = "medium"
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_xlabel("Epoch", fontsize=text_fontsize)
    ax.set_ylabel("Score", fontsize=text_fontsize)
    ax.grid(True)
    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))
    ax.xaxis.set_major_locator(ticker.MultipleLocator(30))
    ax.fill_between(epochs, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1, color="r")
    ax.fill_between(epochs, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(epochs, train_scores_mean, '-', color="r", linewidth=2.0, label="Train")
    ax.plot(epochs, test_scores_mean, '-', color="g", linewidth=2.0, label="Test")
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    x1, x2, y1, y2 = plt.axis()
    plt.axis((x1, x2, 0.0, 1.0))
    plt.savefig(save_path+"mean_acc.png", dpi=300)
    plt.show()
plot_mean_acc(history)
plt.clf()

print("Plotting the mean loss")
def plot_mean_loss(history):
    train_scores = np.zeros((len(history), len(history[0].history['loss'])))
    for fold in range(len(history)):
        train_scores[fold] = history[fold].history['loss']
    test_scores = np.zeros((len(history), len(history[0].history['val_loss'])))
    for fold in range(len(history)):
        test_scores[fold] = history[fold].history['val_loss']
    epochs = np.linspace(0, len(history[0].history['loss']), len(history[0].history['loss']))
    train_scores_mean = np.mean(train_scores, axis=0)
    train_scores_std = np.std(train_scores, axis=0)
    test_scores_mean = np.mean(test_scores, axis=0)
    test_scores_std = np.std(test_scores, axis=0)

    figsize = (14, 6)
    text_fontsize = "small"
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_xlabel("Epoch", fontsize=text_fontsize)
    ax.set_ylabel("Score", fontsize=text_fontsize)
    ax.grid(True)
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.xaxis.set_major_locator(ticker.MultipleLocator(30))
    ax.fill_between(epochs, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1, color="r")
    ax.fill_between(epochs, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(epochs, train_scores_mean, '-', color="r", linewidth=2.0, label="Train")
    for j in range(len(test_scores_mean)):
        if test_scores_mean[j] > 5:
            test_scores_mean[j] = 5
    ax.plot(epochs, test_scores_mean, '-', color="g", linewidth=2.0, label="Test")
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    plt.savefig(save_path+"mean_loss", dpi=300)
    plt.show()
plot_mean_loss(history)