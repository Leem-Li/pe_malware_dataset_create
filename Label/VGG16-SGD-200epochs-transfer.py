# 使用VGG的notop层，在top层用flatten ——> fc1(4096) ——> fc2(4096) ——> Dense(9)，只重训练最后的3层

import os
import os.path
import glob
import time
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.cm as colormap

import numpy as np
np.random.seed(1)

import keras

from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.preprocessing.image import img_to_array
from keras.applications.imagenet_utils import preprocess_input

from keras.callbacks import ModelCheckpoint,EarlyStopping
from keras.models import Sequential,Model
from keras.layers import Input,Flatten,Dense,Dropout,GlobalAveragePooling2D,Conv2D,MaxPooling2D

os.environ['CUDA_VISIBLE_DEVICES'] = '3'

# imagedir = "/home/limingzhao/image-classification-models/new-14-family-111"
imagedir = sys.argv[1]
epoch = 200
save_parameters =sys.argv[1]+"/Malimg-vgg16-SGD-500epochs-transfer-0.8.h5"
# save_parameters = '/home/limingzhao/image-classification-models/mine/vgg16-200/Malimg-vgg16-SGD-500epochs-transfer-0.8.h5'
#X_train_drop = 0.9
# Experiment_results = "/home/limingzhao/image-classification-models/mine/vgg16-200/"
os.mkdir(sys.argv[1]+"/png")
Experiment_results = sys.argv[1]+"/png/"


def preprocess_input(x):
    x /= 255.
    x -= 0.5
    x *= 2.
    return x

cur_dir = os.getcwd()
os.chdir(imagedir)  # the parent folder with sub-folders

# Get number of samples per family
list_fams = sorted(os.listdir(os.getcwd()), key=str.lower)  # vector of strings with family names
no_imgs = []  # No. of samples per family
for i in range(len(list_fams)):
    os.chdir(list_fams[i])
    len1 = len(glob.glob('*.png'))  # 取百分之x，样本数向下取整
    no_imgs.append(len1)
    os.chdir('..')
num_samples = np.sum(no_imgs)  # total number of all samples

# Compute the labels
y = np.zeros(num_samples)
pos = 0
label = 0
for i in no_imgs:
    print ("Label:%2d\tFamily: %15s\tNumber of images: %d" % (label, list_fams[label], i))
    for j in range(i):
        y[pos] = label
        pos += 1
    label += 1
num_classes = label


# Compute the features
width, height,channels = (224,224,3)
X = np.zeros((num_samples, width, height, channels))
cnt = 0
list_paths = [] # List of image paths
print("Processing images ...")
for i in range(len(list_fams)):
    pro = 1
    for img_file in glob.glob(list_fams[i]+'/*.JPG'):
        if pro > no_imgs[i]:
            break
        #print("[%d] Processing image: %s" % (cnt, img_file))
        list_paths.append(os.path.join(os.getcwd(),img_file))
        img = image.load_img(img_file, target_size=(224, 224))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis=0)
        x = preprocess_input(x)
        X[cnt] = x
        cnt += 1
        pro += 1
print("Images processed: %d" %(cnt))


os.chdir(cur_dir)

# Encoding classes (y) into integers (y_encoded) and then generating one-hot-encoding (Y)
encoder = LabelEncoder()
encoder.fit(y)
y_encoded = encoder.transform(y)
Y = np_utils.to_categorical(y_encoded)

# Creating base_model (VGG16 notop)
image_shape = (224, 224, 3)
base_model = VGG16(weights='imagenet', input_shape=image_shape, include_top=False)

#filename = '/home/mayixuan/mal-visual/extracted-features/BIG15-vgg16features.npy'
#if os.path.exists(filename):
#    print("Loading VGG16 extracted features from %s ..." %(filename))
#    vgg16features = np.load(filename)
#else:
#    print("Extracting features from VGG16 layers ...")
#    vgg16features = base_model.predict(X)
#    print("Saving VGG16 extracted features into %s ..." %(filename))
#    np.save(filename, vgg16features)

# Create stratified k-fold subsets
kfold = 10  # no. of folds
skf = StratifiedKFold(kfold, shuffle=True,random_state=1)
skfind = [None] * kfold  # skfind[i][0] -> train indices, skfind[i][1] -> test indices
cnt = 0
for index in skf.split(X, y):
    skfind[cnt] = index
    cnt += 1

# Training top_model and saving min training loss weights
num_epochs = epoch
history = []
conf_mat = np.zeros((len(list_fams), len(list_fams)))  # Initializing the Confusion Matrix
#early = EarlyStopping(monitor='val_loss', patience=13, mode='min', restore_best_weights=True)
checkpointer = ModelCheckpoint(
    filepath=save_parameters,
    monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')
all_time = 0
for i in range(kfold):
    train_indices = skfind[i][0]
    test_indices = skfind[i][1]
    #X_train = vgg16features[train_indices]
    X_train = X[train_indices]
    Y_train = Y[train_indices]
    #X_test = vgg16features[test_indices]
    X_test = X[test_indices]
    Y_test = Y[test_indices]
    y_test = y[test_indices]

    #X_train, X_drop, Y_train, Y_drop = train_test_split(X_train, Y_train, test_size=X_train_drop, random_state=0)
    # 仅取训练集的80%用于训练，测试集统一
    index = [j for j in range(len(X_train))]
    np.random.shuffle(index)
    X_train = X_train[index]
    Y_train = Y_train[index]
    x = base_model.output
    x = Flatten(name='flatten')(x)  # input_shape=(7,7,512)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)
    vgg_model = Model(inputs=base_model.input, outputs=predictions)  # 网络链接要注意

    #vgg_model = Sequential()
    #vgg_model.add(base_model)
    #vgg_model.add(Flatten(name='flatten'))
    #vgg_model.add(Dense(4096, activation='relu', name='fc1'))
    #vgg_model.add(Dense(4096, activation='relu', name='fc2'))
    #vgg_model.add(Dense(num_classes, activation='softmax', name='predictions'))


    # 冻结前面，只重训练最后3个全连接层
    for layer in vgg_model.layers[1:19]:
        print(layer.name, layer)
        layer.trainable = False

    #print(vgg_model.summary())

    vgg_model.compile(optimizer=keras.optimizers.SGD(lr=5e-6, momentum=0.9), loss='categorical_crossentropy',
                      metrics=['accuracy'])

    start = time.time()
    h = vgg_model.fit(X_train, Y_train, validation_split=0.25, epochs=num_epochs,
                      batch_size=64, verbose=1, callbacks=[checkpointer])
    end = time.time()
    history.append(h)


    start_2 = time.time()
    y_prob = vgg_model.predict(X_test, verbose=2)  # Testing
    y_pred = np.argmax(y_prob, axis=1)
    end_2 = time.time()

    print("[%d] Test acurracy: %.4f (%.4f s)" % (i, accuracy_score(y_test, y_pred), end - start))
    # 时间开销
    print()
    print()
    print('-----------------------------------------------------------------------')
    print('第 %d 次交叉验证' % (i + 1))
    print("训练一个fold用时: %.4f s" % (end - start))
    per_fold_time = end-start
    all_time += per_fold_time
    print("预测 %d 个样本用时 : %.4f s" % (len(X_test), end_2 - start_2))
    # 一个fold的分类性能
    print(classification_report(y_test, y_pred, digits=4))
    print('此fold的混淆矩阵：')
    print()

    cm = confusion_matrix(y_test, y_pred)  # Compute confusion matrix for this fold
    print(cm)
    conf_mat = conf_mat + cm  # Compute global confusion matrix


# Computing the average accuracy
avg_acc = np.trace(conf_mat)/sum(no_imgs)
print("Average acurracy: %.4f" %(avg_acc))
print("All time: %.4f" % (all_time))

# plot the graph
save_path=Experiment_results



print("Plotting the accuracy")
def plot_acc(history):
    figure = plt.gcf()
    figure.set_size_inches(14, 6)
    ax = plt.subplot()
    #plt.title('Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))
    for i in range(len(history)):
        color=next(colors)
        plt.plot(history[i].history['accuracy'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)
        plt.plot(history[i].history['val_accuracy'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)
    x1, x2, y1, y2 = plt.axis()
    plt.axis((x1, x2, 0.0, 1.0))
    plt.legend()
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    plt.grid(True)
    plt.savefig(save_path + "accuracy.png",dpi=300)
    plt.show()
plot_acc(history)

plt.clf()


print("Plotting the loss")
def plot_loss(history):
    figure = plt.gcf()
    figure.set_size_inches(14, 6)
    ax = plt.subplot()
    #plt.title('Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))
    for i in range(len(history)):
        color=next(colors)
        plt.plot(history[i].history['loss'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)
        for j in range(len(history[i].history['val_loss'])):
            if (history[i].history['val_loss'])[j] > 5:
                (history[i].history['val_loss'])[j] = 2
        plt.plot(history[i].history['val_loss'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)
    plt.legend()
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    plt.grid(True)
    plt.savefig(save_path + "loss.png", dpi=300)
    plt.show()
plot_loss(history)

plt.clf()


print("Plotting the confusion matrix")
figure = plt.gcf()
figure.set_size_inches(18, 12)
plt.imshow(conf_mat,interpolation='nearest',cmap='coolwarm')
for row in range(len(list_fams)):
    for col in range(len(list_fams)):
        plt.annotate(str(int(conf_mat[row][col])),xy=(col,row),ha='center',va='center')
plt.xticks(range(len(list_fams)),list_fams,rotation=90,fontsize=10)
plt.yticks(range(len(list_fams)),list_fams,fontsize=10)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.savefig(save_path + "confusion_matrix.png", dpi=300)
plt.show()

plt.clf()


print("Plotting the confusion matrix normalized")
conf_mat_norm = conf_mat/no_imgs  # Normalizing the confusion matrix
conf_mat_norm = np.around(conf_mat_norm,decimals=2)  # rounding to display in figure
figure = plt.gcf()
figure.set_size_inches(20, 12)
plt.imshow(conf_mat_norm,interpolation='nearest',cmap='seismic')
for row in range(len(list_fams)):
    for col in range(len(list_fams)):
        plt.annotate(str(conf_mat_norm[row][col]),xy=(col,row),ha='center',va='center')
plt.xticks(range(len(list_fams)),list_fams,rotation=90,fontsize=10)
plt.yticks(range(len(list_fams)),list_fams,fontsize=10)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.savefig(save_path + "confusion_matrix_normalized.png", dpi=300)
plt.show()

plt.clf()

import seaborn as sns

print("Plotting the gray confusion matrix normalized")
conf_mat_norm = conf_mat/np.sum(conf_mat,axis=1)  # Normalizing the confusion matrix
conf_mat_norm = np.around(conf_mat_norm,decimals=2)  # rounding to display in figure

figure = plt.gcf()
figure.set_size_inches(20, 12)
sns.set(font_scale=1.25)
hm = sns.heatmap(conf_mat_norm, cbar=False, annot=True, square=True,
                 fmt='.2f', annot_kws={'size': 9}, linewidth = 0.1, cmap = 'binary',
                 yticklabels=list_fams, xticklabels=list_fams)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.savefig(save_path + "gray_confusion_matrix_normalized.png", dpi=300)
plt.show()

plt.clf()

import matplotlib.ticker as ticker
plt.style.use('seaborn-whitegrid')

print("Plotting the mean accuracy")
def plot_mean_acc(history):
    train_scores = np.zeros((len(history), len(history[0].history['accuracy'])))
    for fold in range(len(history)):
        train_scores[fold] = history[fold].history['accuracy']
    test_scores = np.zeros((len(history), len(history[0].history['val_accuracy'])))
    for fold in range(len(history)):
        test_scores[fold] = history[fold].history['val_accuracy']
    epochs = np.linspace(0, len(history[0].history['accuracy']), len(history[0].history['accuracy']))
    train_scores_mean = np.mean(train_scores, axis=0)
    train_scores_std = np.std(train_scores, axis=0)
    test_scores_mean = np.mean(test_scores, axis=0)
    test_scores_std = np.std(test_scores, axis=0)

    figsize = (14, 6)
    text_fontsize = "medium"
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_xlabel("Epoch", fontsize=text_fontsize)
    ax.set_ylabel("Score", fontsize=text_fontsize)
    ax.grid(True)
    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))
    ax.xaxis.set_major_locator(ticker.MultipleLocator(30))
    ax.fill_between(epochs, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1, color="r")
    ax.fill_between(epochs, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(epochs, train_scores_mean, '-', color="r", linewidth=2.0, label="Train")
    ax.plot(epochs, test_scores_mean, '-', color="g", linewidth=2.0, label="Test")
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    x1, x2, y1, y2 = plt.axis()
    plt.axis((x1, x2, 0.0, 1.0))
    plt.savefig(save_path+"mean_acc.png", dpi=300)
    plt.show()
plot_mean_acc(history)

plt.clf()


print("Plotting the mean loss")
def plot_mean_loss(history):
    train_scores = np.zeros((len(history), len(history[0].history['loss'])))
    for fold in range(len(history)):
        train_scores[fold] = history[fold].history['loss']
    test_scores = np.zeros((len(history), len(history[0].history['val_loss'])))
    for fold in range(len(history)):
        test_scores[fold] = history[fold].history['val_loss']
    epochs = np.linspace(0, len(history[0].history['loss']), len(history[0].history['loss']))
    train_scores_mean = np.mean(train_scores, axis=0)
    train_scores_std = np.std(train_scores, axis=0)
    test_scores_mean = np.mean(test_scores, axis=0)
    test_scores_std = np.std(test_scores, axis=0)

    figsize = (14, 6)
    text_fontsize = "x-small"
    fig, ax = plt.subplots(1, 1, figsize=figsize)
    ax.set_xlabel("Epoch", fontsize=text_fontsize)
    ax.set_ylabel("Score", fontsize=text_fontsize)
    ax.grid(True)
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.xaxis.set_major_locator(ticker.MultipleLocator(30))
    ax.fill_between(epochs, train_scores_mean - train_scores_std,
                    train_scores_mean + train_scores_std, alpha=0.1, color="r")
    ax.fill_between(epochs, test_scores_mean - test_scores_std,
                    test_scores_mean + test_scores_std, alpha=0.1, color="g")
    ax.plot(epochs, train_scores_mean, '-', color="r", linewidth=2.0, label="Train")
    for j in range(len(test_scores_mean)):
        if test_scores_mean[j] > 5:
            test_scores_mean[j] = 2
    ax.plot(epochs, test_scores_mean, '-', color="g", linewidth=2.0, label="Test")
    ax.tick_params(labelsize=text_fontsize)
    ax.legend(loc="best", fontsize=text_fontsize)
    plt.savefig(save_path+"mean_loss", dpi=300)
    plt.show()
plot_mean_loss(history)